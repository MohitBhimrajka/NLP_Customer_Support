{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf2b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d05f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and prepared.\n",
      "Total number of tickets to index: 8469\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Product setup | issue productpurchased please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Peripheral compatibility | issue productpurcha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Network problem | facing problem productpurcha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Account access | issue productpurchased please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Data loss | issue productpurchased please assi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticket ID                                      combined_text\n",
       "0          1  Product setup | issue productpurchased please ...\n",
       "1          2  Peripheral compatibility | issue productpurcha...\n",
       "2          3  Network problem | facing problem productpurcha...\n",
       "3          4  Account access | issue productpurchased please...\n",
       "4          5  Data loss | issue productpurchased please assi..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the processed data from Notebook 1\n",
    "df = pd.read_parquet('processed_customer_support_data.parquet')\n",
    "\n",
    "# 2. Create the combined text feature for finding duplicates\n",
    "# We'll use the original 'Ticket Subject' for its clean signal\n",
    "df['combined_text'] = df['Ticket Subject'] + \" | \" + df['Cleaned_Description']\n",
    "\n",
    "# Let's also keep the original Ticket ID for easy reference\n",
    "df = df[['Ticket ID', 'combined_text']].copy()\n",
    "\n",
    "print(\"Data loaded and prepared.\")\n",
    "print(f\"Total number of tickets to index: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93f7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF Vectorizer...\n",
      "TF-IDF matrix created.\n",
      "Shape of the matrix: (8469, 6030)\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and fit the TF-IDF Vectorizer\n",
    "print(\"Fitting TF-IDF Vectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "print(\"TF-IDF matrix created.\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\") # (num_tickets, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bc2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the search function\n",
    "def find_tfidf_duplicates(ticket_id: int, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Finds the most similar tickets to a given ticket_id using TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    # Get the index of the ticket from its ID\n",
    "    try:\n",
    "        ticket_idx = df.index[df['Ticket ID'] == ticket_id].tolist()[0]\n",
    "    except IndexError:\n",
    "        return f\"Error: Ticket ID {ticket_id} not found.\"\n",
    "    \n",
    "    # Get the TF-IDF vector for our query ticket\n",
    "    query_vector = tfidf_matrix[ticket_idx]\n",
    "    \n",
    "    # Calculate cosine similarity between the query and all other tickets\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    \n",
    "    # Flatten the scores array and get the indices of the top N scores\n",
    "    # We use [::-1] to sort in descending order\n",
    "    top_indices = np.argsort(similarity_scores[0])[-top_n-1:-1][::-1]\n",
    "    \n",
    "    # Get the similarity scores for the top indices\n",
    "    top_scores = similarity_scores[0][top_indices]\n",
    "    \n",
    "    # Get the original ticket details for the top indices\n",
    "    results_df = df.iloc[top_indices].copy()\n",
    "    results_df['similarity_score'] = top_scores\n",
    "    \n",
    "    print(f\"--- Query Ticket ---\")\n",
    "    print(f\"ID: {ticket_id}\")\n",
    "    print(f\"Text: {df.iloc[ticket_idx]['combined_text']}\\n\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8322e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query Ticket ---\n",
      "ID: 20\n",
      "Text: Software bug | issue productpurchased please assist issue productpurchased please assist customer reviewer husband able take order apple ive checked available software update productpurchased none\n",
      "\n",
      "--- Top 5 Duplicates (TF-IDF) ---\n",
      "      Ticket ID                                      combined_text  \\\n",
      "730         731  Software bug | issue productpurchased please a...   \n",
      "5008       5009  Payment issue | issue productpurchased please ...   \n",
      "8456       8457  Payment issue | issue productpurchased please ...   \n",
      "559         560  Display issue | issue productpurchased please ...   \n",
      "6479       6480  Display issue | issue productpurchased please ...   \n",
      "\n",
      "      similarity_score  \n",
      "730           0.535271  \n",
      "5008          0.459492  \n",
      "8456          0.459492  \n",
      "559           0.455732  \n",
      "6479          0.455732  \n"
     ]
    }
   ],
   "source": [
    "# --- 3. Test the TF-IDF system ---\n",
    "\n",
    "# Let's find a ticket about a common issue, for example, a \"software bug\"\n",
    "# (We'll have to find a valid Ticket ID from our data first)\n",
    "# Let's assume Ticket ID 20 is a good example (you can change this ID)\n",
    "try:\n",
    "    results = find_tfidf_duplicates(ticket_id=20)\n",
    "    print(\"--- Top 5 Duplicates (TF-IDF) ---\")\n",
    "    print(results)\n",
    "except TypeError:\n",
    "    # This will happen if the ticket_id is not found.\n",
    "    # Let's find a valid ID and try again.\n",
    "    sample_id = df['Ticket ID'].iloc[20] # Get the ID of the 21st ticket\n",
    "    print(f\"Testing with a valid ID: {sample_id}\\n\")\n",
    "    results = find_tfidf_duplicates(ticket_id=sample_id)\n",
    "    print(\"--- Top 5 Duplicates (TF-IDF) ---\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fa00d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- 1. Load a fine-tuned Transformer model ---\n",
    "# We'll use the base model from our gatekeeper, which has been fine-tuned on our data.\n",
    "# We load it with AutoModel to get the hidden states, not the classification head.\n",
    "model_name = \"best-gatekeeper-model\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22217f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for all tickets...\n",
      "Embeddings generated.\n",
      "Shape of the embedding matrix: (8469, 768)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Generate Embeddings for all tickets ---\n",
    "# This function will take a batch of texts and return their embeddings\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()} # Move inputs to the correct device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # We use the mean of the last hidden state as the sentence embedding\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Generate embeddings for all tickets in the dataframe\n",
    "# This may take a few minutes to run\n",
    "print(\"Generating embeddings for all tickets...\")\n",
    "# We'll do this in batches to manage memory\n",
    "batch_size = 32\n",
    "all_embeddings = np.vstack([get_embeddings(df['combined_text'][i:i+batch_size].tolist()) for i in range(0, len(df), batch_size)])\n",
    "print(\"Embeddings generated.\")\n",
    "print(f\"Shape of the embedding matrix: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7ef57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query Ticket ---\n",
      "ID: 21\n",
      "Text: Payment issue | issue productpurchased please assist name microsoft surface pro version usage ive checked available software update productpurchased none\n",
      "\n",
      "--- Top 5 Duplicates (Transformer Embeddings) ---\n",
      "      Ticket ID                                      combined_text  \\\n",
      "3484       3485  Data loss | issue productpurchased please assi...   \n",
      "3980       3981  Hardware issue | issue productpurchased please...   \n",
      "7713       7714  Network problem | issue productpurchased pleas...   \n",
      "4159       4160  Display issue | issue productpurchased please ...   \n",
      "4717       4718  Network problem | issue productpurchased pleas...   \n",
      "\n",
      "      similarity_score  \n",
      "3484          0.997018  \n",
      "3980          0.996951  \n",
      "7713          0.996826  \n",
      "4159          0.996800  \n",
      "4717          0.996796  \n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create the new search function ---\n",
    "def find_embedding_duplicates(ticket_id: int, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Finds the most similar tickets to a given ticket_id using Transformer embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ticket_idx = df.index[df['Ticket ID'] == ticket_id].tolist()[0]\n",
    "    except IndexError:\n",
    "        return f\"Error: Ticket ID {ticket_id} not found.\"\n",
    "    \n",
    "    query_embedding = all_embeddings[ticket_idx].reshape(1, -1)\n",
    "    \n",
    "    similarity_scores = cosine_similarity(query_embedding, all_embeddings)\n",
    "    \n",
    "    top_indices = np.argsort(similarity_scores[0])[-top_n-1:-1][::-1]\n",
    "    top_scores = similarity_scores[0][top_indices]\n",
    "    \n",
    "    results_df = df.iloc[top_indices].copy()\n",
    "    results_df['similarity_score'] = top_scores\n",
    "    \n",
    "    print(f\"--- Query Ticket ---\")\n",
    "    print(f\"ID: {ticket_id}\")\n",
    "    print(f\"Text: {df.iloc[ticket_idx]['combined_text']}\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# --- 4. Test the new system on the same ticket ---\n",
    "sample_id = df['Ticket ID'].iloc[20] # Get the same ID as before\n",
    "results_embeddings = find_embedding_duplicates(ticket_id=sample_id)\n",
    "print(\"--- Top 5 Duplicates (Transformer Embeddings) ---\")\n",
    "print(results_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
